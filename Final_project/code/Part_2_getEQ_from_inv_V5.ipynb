{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the inventory data for the following criterias\n",
    "Select an earthquake event (by id).\\\n",
    "Get the station inventory within the study area which recorded this event.\\\n",
    "Use this station inventory data to download earthquake data (`.mseed`) and station data (`.txt` or `.xml` format).\\\n",
    "3-channel data for earthquakes are along x, y and z axis. Usually named as HHE, HHN and HHZ or HH1, HH2, HHZ.\\\n",
    "I want to select the following channels in order. If the 1st one is available, get that one only and stop. If not search for the 2nd one and so on.\n",
    "1. HH*\n",
    "2. BH*\n",
    "3. HN*\n",
    "4. EH* \\\n",
    "Lastly, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from obspy.clients.fdsn import Client\n",
    "from obspy import UTCDateTime, read_inventory, Inventory, read, Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           id   mag                              time\n",
      "0  nc72086051  4.94  2013-10-11 23:05:37.330000+00:00\n",
      "Selected event index: 0\n",
      "Selected event: nc72086051, mag: 4.94 in grid 3\n",
      "Event nc72086051 already downloaded. Skipping...\n",
      "           id   mag                              time\n",
      "0  nc73139366  4.19  2019-02-03 23:43:21.810000+00:00\n",
      "1  nc73139111  4.30  2019-02-02 10:52:21.990000+00:00\n",
      "2  nc73201181  5.58  2019-06-23 03:53:02.890000+00:00\n",
      "3  nc73133256  4.08  2019-01-15 20:20:20.120000+00:00\n",
      "4  nc72820761  4.03  2017-06-24 21:22:03.060000+00:00\n",
      "Selected event index: 2\n",
      "Selected event: nc73201181, mag: 5.58 in grid 6\n"
     ]
    }
   ],
   "source": [
    "# Read earthquake data\n",
    "eqdf = pd.read_csv(\"../data/above_slab_eq_df.csv\", parse_dates=[\"time\"])\n",
    "eqdf4 = eqdf[eqdf.mag >= 4].reset_index(drop=True)\n",
    "\n",
    "# Extract unique grid codes\n",
    "unique_grid_codes = eqdf4['grid_code'].unique()\n",
    "\n",
    "# Loop through each grid code\n",
    "for i in range(len(unique_grid_codes)):\n",
    "    temp_df = eqdf4[eqdf4.grid_code == unique_grid_codes[i]].reset_index(drop=True) # select events in the grid\n",
    "\n",
    "    # Check if there are events in the grid\n",
    "    if len(temp_df) > 0:\n",
    "        print(temp_df[['id', 'mag', 'time']])\n",
    "        # Select the event with maximum magnitude\n",
    "        eq_idx = temp_df['mag'].idxmax()\n",
    "        print(f\"Selected event index: {eq_idx}\")\n",
    "\n",
    "        # get the details of the event\n",
    "        eq = temp_df.iloc[eq_idx]\n",
    "        event_id = eq['id']\n",
    "        print(f\"Selected event: {event_id}, mag: {eq['mag']} in grid {unique_grid_codes[i]}\")\n",
    "\n",
    "        # if an event is already downloaded, skip\n",
    "        if os.path.exists(f\"../data/eq_data/{event_id}/inventory/station_inventory_{event_id}.xml\"):\n",
    "            print(f\"Event {event_id} already downloaded. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Get event time\n",
    "        event_time = UTCDateTime(pd.to_datetime(eq.time))\n",
    "        starttime = event_time - 30\n",
    "        endtime = event_time + 120\n",
    "\n",
    "        # define the datacenters and channel list\n",
    "        client_list = ['IRIS', 'NCEDC', 'SCEDC']\n",
    "        channel_list = 'HH*,BH*,HN*,EH*' # select broadband and high sample rate channels\n",
    "\n",
    "        # Create a folder for the event\n",
    "        output_folder = f\"../data/eq_data/{event_id}/inventory\"\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        merged_inventory = Inventory()\n",
    "\n",
    "        # Loop through each client (IRIS, NCEDC, SCEDC data centers)\n",
    "        for client_name in client_list:\n",
    "            client = Client(client_name, debug=False, timeout=3600)\n",
    "            try:\n",
    "                inv = client.get_stations(\n",
    "                    network=\"*\",\n",
    "                    station=\"*\",\n",
    "                    location=\"*\",\n",
    "                    channel=channel_list,\n",
    "                    starttime=starttime,\n",
    "                    endtime=endtime,\n",
    "                    level=\"channel\",\n",
    "                    minlatitude=39.75,\n",
    "                    maxlatitude=41.5,\n",
    "                    minlongitude=-125,\n",
    "                    maxlongitude=-123,\n",
    "                )\n",
    "                merged_inventory.networks.extend(inv.networks)\n",
    "                merged_inventory = merged_inventory.remove(network=\"SY\")\n",
    "\n",
    "                # Write the inventory to files\n",
    "                # inv.write(f\"{output_folder}/{event_id}_{client_name}.xml\", format=\"STATIONXML\")\n",
    "                # inv.write(f\"{output_folder}/{event_id}_{client_name}.txt\", format=\"STATIONTXT\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching data from {client_name}: {e}\")    \n",
    "\n",
    "        break # test with only one grid\n",
    "        \n",
    "        # write the whole inventory to a file\n",
    "        merged_inventory.write(f\"{output_folder}/station_inventory_{event_id}.xml\", format=\"STATIONXML\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"No events in grid code {unique_grid_codes[i]}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the inventory files already downloaded \n",
    "# And download `.mseed` and station_data (`.xml & .txt`)\n",
    "\n",
    "Here I will read the inventory file which contain details about all the stations that recorded a particular earthquake event. \\\n",
    "From that inventory file I will get all the necessary informations I need to download the seismic data (a numpy timeseries in `.mseed` format). \\\n",
    "I will also download the metadata for that record in `.xml & .txt` formats.\\\n",
    "\n",
    "This process will use `multiprocessing.Pool.imap_unordered` module for paraller processing of the download.\\\n",
    "For the code see `./code/my_funcs/get_waveforms_parallel_v3.py` where I defined the download fuction combined with parallel processing. \\\n",
    "This significantly improves the runtime.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Getting data for event nc73201181\n",
      "Fetching data from NCEDC...\n",
      "Fetching data from IRIS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 108/108 [00:25<00:00,  4.24it/s]\n",
      "/Users/mdarifulislam/miniconda3/envs/obspy/lib/python3.10/site-packages/obspy/io/mseed/core.py:823: UserWarning: File will be written with more than one different record lengths.\n",
      "This might have a negative influence on the compatibility with other programs.\n",
      "  warnings.warn(msg % 'record lengths')\n"
     ]
    }
   ],
   "source": [
    "# reload the module to get the latest changes\n",
    "import sys\n",
    "sys.path.append('./my_funcs')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "import glob\n",
    "import os \n",
    "# import all the `get_waveforms` function\n",
    "from my_funcs.get_waveforms_parallel_v3 import *\n",
    "\n",
    "# define the client list i.e. the data centers to download data from\n",
    "client_list = ['NCEDC', 'IRIS'] #, 'SCEDC']\n",
    "\n",
    "# get a list of all the event id folders\n",
    "event_paths = glob.glob(\"../data/eq_data/*\")\n",
    "event_ids = [os.path.basename(path) for path in event_paths]\n",
    "\n",
    "# Read earthquake data\n",
    "eqdf = pd.read_csv(\"../data/above_slab_eq_df.csv\", parse_dates=[\"time\"])\n",
    "\n",
    "# define the priority channels\n",
    "priority_channels = ['HH*', 'BH*', 'HN*', 'EH*']\n",
    "\n",
    "event_ids = ['nc73201181'] # test with one event #################################### change it ####################\n",
    "# loop through each event id and download the data\n",
    "for event_id in event_ids:\n",
    "\n",
    "    # define the output folder\n",
    "    output_folder = f\"../data/eq_data/{event_id}/\"\n",
    "\n",
    "    # check if the event data is already downloaded\n",
    "    if os.path.exists(f\"../data/eq_data/{event_id}/{event_id}.mseed\"):\n",
    "        print(f\"Event {event_id} already downloaded. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Getting data for event {event_id}\")\n",
    "\n",
    "    # Read the inventory\n",
    "    inventory = read_inventory(f\"../data/eq_data/{event_id}/inventory/station_inventory_{event_id}.xml\")\n",
    "\n",
    "    #get the event time, start time and end time\n",
    "    eq = eqdf[eqdf.id == event_id] # get the event details\n",
    "    event_time = UTCDateTime(pd.to_datetime(eq.time.values[0])) # get the event time in UTC format\n",
    "    starttime = event_time - 30 # start time is 30 seconds before the event time\n",
    "    endtime = event_time + 120 # end time is 120 seconds after the event time\n",
    "\n",
    "    # Call the function with the desired parameters\n",
    "    # this will downaload and write the data to a file, to change path, edit the function\n",
    "    get_waveforms_parallel(client_list, inventory, starttime, endtime, output_folder, priority_channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "obspy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
